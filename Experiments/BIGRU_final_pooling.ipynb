{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"EWcymo_FWADn"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from keras import Sequential\n","from keras.utils import Sequence\n","from keras.layers import LSTM, Dense, Masking, GRU\n","import numpy as np\n","import keras\n","from keras.utils import np_utils\n","from keras import optimizers\n","from keras.models import Sequential, Model\n","from keras.layers import Embedding, Dense, Input, concatenate, Layer, Lambda, Dropout, Activation\n","import datetime\n","from datetime import datetime\n","from keras.callbacks import ModelCheckpoint, EarlyStopping, Callback, TensorBoard\n","from keras.callbacks import ReduceLROnPlateau\n","from keras.models import load_model\n","import tensorflow as tf\n","import tensorflow_hub as hub\n","import numpy as np\n","from numpy import load\n","import pandas as pd\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import confusion_matrix\n","\n","\n","np.random.seed(1337)# setting the random seed value"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21248,"status":"ok","timestamp":1679392986355,"user":{"displayName":"Prameela Madambakam","userId":"17970938217400203646"},"user_tz":-330},"id":"8ZULOpIvF4Yt","outputId":"d2edcf60-db52-4a1c-e367-be9a15e0cc66"},"outputs":[],"source":["# Mounting Drive\n","# from google.colab import drive\n","# drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"e2RgVj0jFrFp"},"outputs":[],"source":["# path_dataset = \"drive/My Drive/DL_project_LJP/ILDC_multi.csv\" # path to dataset"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"fwfyU5uCRR0K"},"outputs":[],"source":["dataset = pd.read_csv('ILDC_multi.csv')"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"_1YqIPZ7L2K5"},"outputs":[],"source":["# path to transformer generated chunk embeddings eg. XLNet etc.\n","path_transformer_chunk_embeddings_train = 'XLNet_full/XLNet_train.npy' \n","path_transformer_chunk_embeddings_dev = 'XLNet_full/XLNet_dev.npy'\n","path_transformer_chunk_embeddings_test = 'XLNet_full/XLNet_test.npy'"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"c5rkC-VzPPBa"},"outputs":[],"source":["# loading the chunk embeddings\n","x_train0 = load(path_transformer_chunk_embeddings_train, allow_pickle = True)\n","x_dev0 = load(path_transformer_chunk_embeddings_dev, allow_pickle= True)\n","x_test0 = load(path_transformer_chunk_embeddings_test, allow_pickle= True)"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"-aVSH_kLQ3pM"},"outputs":[],"source":["# loading the corresponding label for each case in dataset\n","dev = dataset.loc[dataset['split'] == 'dev'] \n","train = dataset.loc[dataset['split'] == 'train'] \n","test = dataset.loc[dataset['split'] == 'test'] \n","\n","y_train0 = []\n","for i in range(train.shape[0]):\n","    y_train0.append(train.loc[i,'label'])  \n","    \n","y_dev0 = []\n","for i in range(dev.shape[0]):\n","    y_dev0.append(dev.loc[i+32305,'label'])\n","\n","y_test0 = []\n","for i in range(test.shape[0]):\n","    y_test0.append(test.loc[i+33299,'label'])"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Shuffling"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["y_new=y_train0+y_dev0\n","\n","x_new=np.concatenate((x_train0,x_dev0),axis=0)\n","\n","import random\n","list_index=[]\n","for i in range(len(y_new)):\n","  list_index.append(i)\n","\n","print(list_index)\n","random.shuffle(list_index)\n","print(list_index)\n","\n","y_train_new=[]\n","x_train_new=np.empty((0, 768))\n","\n","for i in range(len(y_new)):\n","  y_train_new.append(y_new[list_index[i]])\n","  x_train_new=np.vstack((x_train_new,x_new[list_index[i]]))"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3187,"status":"ok","timestamp":1679393030694,"user":{"displayName":"Prameela Madambakam","userId":"17970938217400203646"},"user_tz":-330},"id":"PnMJO4OQcI5g","outputId":"1074f1b7-b09c-45ae-b42d-fb7b4445034d"},"outputs":[{"ename":"ValueError","evalue":"Input 0 of layer \"max_pooling1d_1\" is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 200)","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[1;32md:\\BTP\\code\\XLNET_BiGRU\\BIGRU_final_pooling.ipynb Cell 10\u001b[0m in \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/BTP/code/XLNET_BiGRU/BIGRU_final_pooling.ipynb#X10sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m encoded_text \u001b[39m=\u001b[39m MaxPooling1D(pool_size\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)(encoded_text) \u001b[39m# Add pooling layer\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/BTP/code/XLNET_BiGRU/BIGRU_final_pooling.ipynb#X10sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m encoded_text1 \u001b[39m=\u001b[39m layers\u001b[39m.\u001b[39mBidirectional(GRU(\u001b[39m100\u001b[39m,))(encoded_text)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/BTP/code/XLNET_BiGRU/BIGRU_final_pooling.ipynb#X10sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m encoded_text2 \u001b[39m=\u001b[39m MaxPooling1D(pool_size\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)(encoded_text1)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/BTP/code/XLNET_BiGRU/BIGRU_final_pooling.ipynb#X10sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# Added a dense layer after encoding\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/BTP/code/XLNET_BiGRU/BIGRU_final_pooling.ipynb#X10sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m out_dense \u001b[39m=\u001b[39m layers\u001b[39m.\u001b[39mDense(\u001b[39m30\u001b[39m, activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrelu\u001b[39m\u001b[39m'\u001b[39m)(encoded_text2)\n","File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n","File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\input_spec.py:232\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    230\u001b[0m     ndim \u001b[39m=\u001b[39m shape\u001b[39m.\u001b[39mrank\n\u001b[0;32m    231\u001b[0m     \u001b[39mif\u001b[39;00m ndim \u001b[39m!=\u001b[39m spec\u001b[39m.\u001b[39mndim:\n\u001b[1;32m--> 232\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    233\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mInput \u001b[39m\u001b[39m{\u001b[39;00minput_index\u001b[39m}\u001b[39;00m\u001b[39m of layer \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mlayer_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    234\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mis incompatible with the layer: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    235\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mexpected ndim=\u001b[39m\u001b[39m{\u001b[39;00mspec\u001b[39m.\u001b[39mndim\u001b[39m}\u001b[39;00m\u001b[39m, found ndim=\u001b[39m\u001b[39m{\u001b[39;00mndim\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    236\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFull shape received: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtuple\u001b[39m(shape)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    237\u001b[0m         )\n\u001b[0;32m    238\u001b[0m \u001b[39mif\u001b[39;00m spec\u001b[39m.\u001b[39mmax_ndim \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    239\u001b[0m     ndim \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mshape\u001b[39m.\u001b[39mrank\n","\u001b[1;31mValueError\u001b[0m: Input 0 of layer \"max_pooling1d_1\" is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 200)"]}],"source":["from keras import layers\n","from keras.layers.pooling import MaxPooling1D\n","\n","# Input layer to convert into required tensor shape\n","text_input = Input(shape=(None,768,), dtype='float32', name='text')\n","# Masking layer to mask the padded values\n","l_mask = layers.Masking(mask_value=-99.)(text_input)\n","# After masking we encoded the vector using 2 bidirectional GRU's\n","encoded_text = layers.Bidirectional(GRU(100,return_sequences=True))(l_mask)\n","\n","# Line for adding 1D pooling layer\n","encoded_text = MaxPooling1D(pool_size=2)(encoded_text) # Add pooling layer\n","\n","encoded_text1 = layers.Bidirectional(GRU(100,))(encoded_text)\n","encoded_text2 = MaxPooling1D(pool_size=2)(encoded_text1)\n","\n","# Added a dense layer after encoding\n","out_dense = layers.Dense(30, activation='relu')(encoded_text2)\n","# And we add a sigmoid classifier on top\n","out = layers.Dense(1, activation='sigmoid')(out_dense)\n","# At model instantiation, we specify the input and the output:\n","model = Model(text_input, out)\n","model.compile(optimizer='Adam',\n","              loss='binary_crossentropy',\n","              metrics=['acc'])\n","model.summary()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YgVbg80acVar"},"outputs":[],"source":["num_sequences = len(x_train0)\n","batch_size = 32 \n","batches_per_epoch =  int(num_sequences/batch_size)\n","num_features= 768\n","def train_generator(): # function to generate batches of corresponding batch size\n","    x_list= x_train0\n","    y_list =  y_train0\n","    # Generate batches\n","    while True:\n","        for b in range(batches_per_epoch):\n","            longest_index = (b + 1) * batch_size - 1\n","            timesteps = len(max(x_train0[:(b + 1) * batch_size][-batch_size:], key=len))\n","            x_train = np.full((batch_size, timesteps, num_features), -99.)\n","            y_train = np.zeros((batch_size,  1))\n","            # padding the vectors with respect to the maximum sequence of each batch and not the whole training data\n","            for i in range(batch_size):\n","                li = b * batch_size + i\n","                x_train[i, 0:len(x_list[li]), :] = x_list[li]\n","                y_train[i] = y_list[li]\n","            yield x_train, y_train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vnGZeO1ieiAQ"},"outputs":[],"source":["num_sequences_val = len(x_dev0)\n","batch_size_val = 32\n","batches_per_epoch_val = int(num_sequences_val/batch_size_val)\n","num_features= 768\n","def val_generator():# Similar function to generate validation batches\n","    x_list= x_dev0\n","    y_list =  y_dev0\n","    # Generate batches\n","    while True:\n","        for b in range(batches_per_epoch_val):\n","            longest_index = (b + 1) * batch_size_val - 1\n","            timesteps = len(max(x_dev0[:(b + 1) * batch_size_val][-batch_size_val:], key=len))\n","            x_train = np.full((batch_size_val, timesteps, num_features), 0)\n","            y_train = np.zeros((batch_size_val,  1))\n","            # padding the vectors with respect to the maximum sequence of each batch and not the whole validation data\n","            for i in range(batch_size_val):\n","                li = b * batch_size_val + i\n","                x_train[i, 0:len(x_list[li]), :] = x_list[li]\n","                y_train[i] = y_list[li]\n","            yield x_train, y_train"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":416},"executionInfo":{"elapsed":634,"status":"error","timestamp":1679393626835,"user":{"displayName":"Prameela Madambakam","userId":"17970938217400203646"},"user_tz":-330},"id":"MYZ7yr9mlYk_","outputId":"6d5a07ad-725e-4839-d16a-d62c887cc2c8"},"outputs":[],"source":["# Setting the callback and training the model\n","call_reduce = ReduceLROnPlateau(monitor='val_acc', factor=0.95, patience=2, verbose=2,mode='auto', min_delta=0.01, cooldown=0, min_lr=0)\n","\n","history= model.fit(train_generator(), steps_per_epoch=batches_per_epoch, epochs=1,validation_data=val_generator(), validation_steps=batches_per_epoch_val, callbacks =[call_reduce] )"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Plot the training and validation accuracy and loss at each epoch\n","import matplotlib.pyplot as plt\n","acc = history.history['acc']\n","val_acc = history.history['val_acc']\n","loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","epochs = range(1, len(acc) + 1)\n","plt.plot(epochs, acc, 'b-', label='Training acc')\n","plt.plot(epochs, val_acc, 'r', label='Validation acc')\n","plt.xlabel('Epochs')\n","plt.ylabel('Accuracy')\n","plt.title('Training and validation accuracy')\n","plt.legend()\n","plt.figure()\n","\n","\n","plt.plot(epochs, loss, 'b-', label='Training loss')\n","plt.plot(epochs, val_loss, 'r', label='Validation loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.title('Training and validation loss')\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y3ET-FUAx8Su"},"outputs":[],"source":["num_sequences_test = len(x_test0)\n","batch_size_test = 32\n","batches_per_epoch_test = int(num_sequences_test/batch_size_test) + 1\n","num_features= 768\n","\n","def test_generator(): # function to generate batches of corresponding batch size\n","    x_list= x_test0\n","    y_list =  y_test0\n","    # Generate batches\n","    while True:\n","        for b in range(batches_per_epoch_test):\n","            if(b == batches_per_epoch_test-1): # An extra if else statement just to manage the last batch as it's size might not be equal to batch size \n","              longest_index = num_sequences_test - 1\n","              timesteps = len(max(x_test0[:longest_index + 1][-batch_size_test:], key=len))\n","              x_train = np.full((longest_index - b*batch_size_test, timesteps, num_features), -99.)\n","              y_train = np.zeros((longest_index - b*batch_size_test,  1))\n","              for i in range(longest_index - b*batch_size_test):\n","                  li = b * batch_size_test + i\n","                  x_train[i, 0:len(x_list[li]), :] = x_list[li]\n","                  y_train[i] = y_list[li]\n","            else:\n","                longest_index = (b + 1) * batch_size_test - 1\n","                timesteps = len(max(x_test0[:(b + 1) * batch_size_test][-batch_size_test:], key=len))\n","                x_train = np.full((batch_size_test, timesteps, num_features), -99.)\n","                y_train = np.zeros((batch_size_test,  1))\n","                # padding the vectors with respect to the maximum sequence of each batch and not the whole test data\n","                for i in range(batch_size_test):\n","                    li = b * batch_size_test + i\n","                    x_train[i, 0:len(x_list[li]), :] = x_list[li]\n","                    y_train[i] = y_list[li]\n","            yield x_train, y_train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i-HzpAFHdD-t"},"outputs":[],"source":["# evaluating on the test data\n","test_loss, test_acc=  model.evaluate_generator(test_generator(), steps= batches_per_epoch_test)\n","print('Test Loss:', test_loss)\n","print('Test Accuracy:', test_acc)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4bko5x4jKnpl"},"outputs":[],"source":["# defining a function which calculates various metrics such as micro and macro precision, accuracy and f1\n","def metrics_calculator(preds, test_labels):\n","    cm = confusion_matrix(test_labels, preds)\n","    TP = []\n","    FP = []\n","    FN = []\n","    for i in range(0,2):\n","        summ = 0\n","        for j in range(0,2):\n","            if(i!=j):\n","                summ=summ+cm[i][j]\n","\n","        FN.append(summ)\n","    for i in range(0,2):\n","        summ = 0\n","        for j in range(0,2):\n","            if(i!=j):\n","                summ=summ+cm[j][i]\n","\n","        FP.append(summ)\n","    for i in range(0,2):\n","        TP.append(cm[i][i])\n","    precision = []\n","    recall = []\n","    for i in range(0,2):\n","        precision.append(TP[i]/(TP[i] + FP[i]))\n","        recall.append(TP[i]/(TP[i] + FN[i]))\n","\n","    macro_precision = sum(precision)/2\n","    macro_recall = sum(recall)/2\n","    micro_precision = sum(TP)/(sum(TP) + sum(FP))\n","    micro_recall = sum(TP)/(sum(TP) + sum(FN))\n","    micro_f1 = (2*micro_precision*micro_recall)/(micro_precision + micro_recall)\n","    macro_f1 = (2*macro_precision*macro_recall)/(macro_precision + macro_recall)\n","    print(\"macro_precision, macro_recall, macro_f1, micro_precision, micro_recall, micro_f1\")\n","    return macro_precision, macro_recall, macro_f1, micro_precision, micro_recall, micro_f1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b49_aClD2TFO"},"outputs":[],"source":["# getting the predicted labels on the test data\n","preds = model.predict_generator(test_generator(), steps= batches_per_epoch_test)\n","y_pred = preds > 0.5\n","\n","print(y_pred)\n","\n","# Calculating all metrics on test data predicted label\n","# y_pred= y_pred.to_tensor()\n","# convert values of y_pred tensor such that true to 1 and false to 0 in tensors\n","# y_pred[y_pred == True] = 1\n","# y_pred[y_pred == False] = 0\n","\n","\n","print(metrics_calculator(y_pred, y_test0[:-1]))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MLGnNSmqLahb"},"outputs":[],"source":["# getting the predicted labels on the dev data\n","preds = model.predict_generator(val_generator(), steps= batches_per_epoch_val)\n","y_pred_dev = preds > 0.5\n","\n","# Calculating all metrics on dev data predicted label\n","print(metrics_calculator(y_pred_dev, y_dev0[:-2]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ITN1ODSfa61U"},"outputs":[],"source":["# saving the trained model\n","model.save('BIGRU_XLNet.h5')  # creates a HDF5 file 'BIGRU_XLNet.h5'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NN5eSAN7R7As"},"outputs":[],"source":["# loading the model\n","# model = load_model('BIGRU_XLNet.h5')"]}],"metadata":{"accelerator":"TPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.7"}},"nbformat":4,"nbformat_minor":0}
